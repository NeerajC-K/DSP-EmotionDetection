{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\aravind\\Anaconda3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import multiprocessing\n",
    "from sklearn import utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=pd.read_excel('Newdataset.xlsx')\n",
    "# Preprocessing thr data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataClean import DataCleaner\n",
    "cleanData = DataCleaner()\n",
    "sentences , emotions = cleanData.cleanData(text[\"Sentence\"]) , text[\"Emotion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    i m already feel somewhat strange give that i ...\n",
       "1    i pm hehehe anyasimbi pm take a nap sweetie pm...\n",
       "2    a boy phone me at night and want to talk to me...\n",
       "3    i a feeling of curious satisfaction to be on t...\n",
       "4                 a breakup with someone i really like\n",
       "Name: Sentence, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.head()\n",
    "#splitting the data into train, test and validation in 80:20 and \n",
    "# test , validation into 50:50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "SEED = 2000\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(sentences, emotions, test_size=.2, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This mthod will tokenize the sentence and append a tag with senetence number\n",
    "# for Word2Vec analysis\n",
    "def labelize_sentences_ug(sentences,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, sentence in zip(sentences.index, sentences):\n",
    "        result.append(TaggedDocument(sentence.split(), [prefix + '_%s' % i]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x = pd.concat([x_train,x_validation,x_test])\n",
    "all_x_w2v = labelize_sentences_ug(all_x, 'all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1091253.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# In Word2Vec, we are using CBOW which will detect target words from\n",
    "# source context words.\n",
    "# size is vector size from maximum number of words\n",
    "# negative is the number of noise words to be drawn\n",
    "# window is the distance between the current word and predicted word\n",
    "# min_count will ignore all the words with frequency less than 2\n",
    "# alpha is the learning rate\n",
    "\n",
    "from DataClean import WordLength\n",
    "wordLength = WordLength()\n",
    "cores = multiprocessing.cpu_count()\n",
    "model_ug_cbow = Word2Vec(sg=0, size=wordLength.getMaxWordLength(all_x), negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_cbow.build_vocab([x.words for x in tqdm(all_x_w2v)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1155519.31it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 739570.94it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1060783.85it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1129717.99it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1104839.98it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1664868.81it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1951746.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1080551.31it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1139963.04it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1051142.54it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 977738.82it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1067271.03it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 942900.43it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1760537.27it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1502186.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# We are training the vectors to predict the word from a sentence by gradually\n",
    "#decreasing the learning rate\n",
    "#This code will take more than 2 hours to execute for 15 epochs\n",
    "sentencesCount = len(all_x_w2v)\n",
    "for epoch in range(15):\n",
    "    model_ug_cbow.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=sentencesCount, epochs=1)\n",
    "    model_ug_cbow.alpha -= 0.002\n",
    "    model_ug_cbow.min_alpha = model_ug_cbow.alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1503712.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# We are build vocabulary of words using Word2Vec with skip gram\n",
    "model_ug_sg = Word2Vec(sg=1, size=wordLength.getMaxWordLength(all_x), negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_sg.build_vocab([x.words for x in tqdm(all_x_w2v)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1671015.26it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1307995.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 2005117.12it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1658133.52it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 990569.88it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 974490.37it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1046709.37it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1753520.44it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1027101.03it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1598682.73it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1789964.29it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 980687.88it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 2005500.62it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1069966.41it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 1920380.93it/s]\n"
     ]
    }
   ],
   "source": [
    "#This code will take more than 2 hours to execute for 15 epochs\n",
    "for epoch in range(15):\n",
    "    model_ug_sg.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_sg.alpha -= 0.002\n",
    "    model_ug_sg.min_alpha = model_ug_sg.alpha\n",
    "\n",
    "# Saving the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_cbow.save('w2v_model_ug_cbow.word2vec')\n",
    "model_ug_sg.save('w2v_model_ug_sg.word2vec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model_ug_cbow = KeyedVectors.load('w2v_model_ug_cbow.word2vec')\n",
    "model_ug_sg = KeyedVectors.load('w2v_model_ug_sg.word2vec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8444 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Creating a dictionary from which ew can extract the word vectors\n",
    "embeddings_index = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    embeddings_index[w] = np.append(model_ug_cbow.wv[w],model_ug_sg.wv[w])\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576138"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numWords = []\n",
    "for sentence in sentences:\n",
    "    numWords.append(len(sentence.split()))\n",
    "vectorLenth = max(numWords) + 5\n",
    "wordCount = 0\n",
    "for i in numWords:\n",
    "    wordCount = wordCount + i\n",
    "wordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to sequence will give a sequential representation of each sentence\n",
    "tokenizer = Tokenizer(num_words=wordCount)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (24000, 80)\n"
     ]
    }
   ],
   "source": [
    "# We are padding the vector eqully with maximum length of the word in a sentence\n",
    "x_train_seq = pad_sequences(sequences, maxlen=vectorLenth)\n",
    "print('Shape of data tensor:', x_train_seq.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_val = tokenizer.texts_to_sequences(x_validation)\n",
    "x_val_seq = pad_sequences(sequences_val, maxlen=vectorLenth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are building a matrix of word vectors, by using word index number\n",
    "# so that our model can refer to the corresponding vector when passed with integer sequence\n",
    "matrixSize = [len(v) for v in embeddings_index.values()][0]\n",
    "embedding_matrix = np.zeros((wordCount, matrixSize))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= wordCount:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.array_equal(embedding_matrix[1326] ,embeddings_index.get('vain'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this we are creating a neural network to train the embedding matrix\n",
    "# which itself can learn the word embeddings as the model trains.\n",
    "# In this way, we are providing first initialization to the embedding layer\n",
    "# so that it can learn more efficiently the task-specifed vectors.\n",
    "# Howver, we are not using this pre trained model  for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aravind\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aravind\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\aravind\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aravind\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 24000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      " - 1425s - loss: 0.2779 - acc: 0.8912 - val_loss: 0.1466 - val_acc: 0.9482\n",
      "Epoch 2/10\n",
      " - 1370s - loss: 0.0699 - acc: 0.9759 - val_loss: 0.1447 - val_acc: 0.9494\n",
      "Epoch 3/10\n",
      " - 1361s - loss: 0.0269 - acc: 0.9910 - val_loss: 0.1707 - val_acc: 0.9484\n",
      "Epoch 4/10\n",
      " - 1886s - loss: 0.0190 - acc: 0.9938 - val_loss: 0.1851 - val_acc: 0.9472\n",
      "Epoch 5/10\n",
      " - 1379s - loss: 0.0156 - acc: 0.9941 - val_loss: 0.2102 - val_acc: 0.9459\n",
      "Epoch 6/10\n",
      " - 5559s - loss: 0.0140 - acc: 0.9942 - val_loss: 0.2541 - val_acc: 0.9388\n",
      "Epoch 7/10\n",
      " - 1533s - loss: 0.0165 - acc: 0.9929 - val_loss: 0.3157 - val_acc: 0.9341\n",
      "Epoch 8/10\n",
      " - 1471s - loss: 0.0142 - acc: 0.9937 - val_loss: 0.3013 - val_acc: 0.9434\n",
      "Epoch 9/10\n",
      " - 3298s - loss: 0.0101 - acc: 0.9948 - val_loss: 0.3117 - val_acc: 0.9393\n",
      "Epoch 10/10\n",
      " - 1535s - loss: 0.0090 - acc: 0.9952 - val_loss: 0.3191 - val_acc: 0.9375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ba750a3b00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code will take more than 2 hours to execute for 10 epochs\n",
    "from DataClean import OneHotEncoding\n",
    "onehotEncoding = OneHotEncoding()\n",
    "model_ptw2v = Sequential()\n",
    "e = Embedding(wordCount, matrixSize ,input_length=vectorLenth)\n",
    "model_ptw2v.add(e)\n",
    "model_ptw2v.add(Flatten())\n",
    "model_ptw2v.add(Dense(512, activation='relu'))\n",
    "model_ptw2v.add(Dense(5, activation='softmax'))\n",
    "model_ptw2v.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_ptw2v.fit(x_train_seq, onehotEncoding.GetOneHotEncodedMatrix(y_train), validation_data=(x_val_seq, onehotEncoding.GetOneHotEncodedMatrix(y_validation)), epochs=10, batch_size=32, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aravind\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.583334\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_ptw2v.evaluate(x_train_seq, onehotEncoding.GetOneHotEncodedMatrix(y_train), verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aravind\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\aravind\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aravind\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 24000 samples, validate on 3000 samples\n",
      "Epoch 1/5\n",
      " - 1390s - loss: 0.1581 - acc: 0.9375 - val_loss: 0.0738 - val_acc: 0.9755\n",
      "Epoch 2/5\n",
      " - 1328s - loss: 0.0563 - acc: 0.9772 - val_loss: 0.0686 - val_acc: 0.9734\n",
      "Epoch 3/5\n",
      " - 1509s - loss: 0.0425 - acc: 0.9820 - val_loss: 0.0816 - val_acc: 0.9697\n",
      "Epoch 4/5\n",
      " - 1314s - loss: 0.0304 - acc: 0.9874 - val_loss: 0.0970 - val_acc: 0.9705\n",
      "Epoch 5/5\n",
      " - 1333s - loss: 0.0249 - acc: 0.9899 - val_loss: 0.1049 - val_acc: 0.9681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ba58572f98>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code will take more than 1 hour to execute for 5 epochs\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "emotionCNNModel = Sequential()\n",
    "e = Embedding(wordCount, matrixSize,input_length=vectorLenth)\n",
    "emotionCNNModel.add(e)\n",
    "# we are adding 100 filters of stride size 1 \n",
    "emotionCNNModel.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "# Global max pooling layer will extract maximum value from each filter by changung\n",
    "# to a one dimensional vector\n",
    "emotionCNNModel.add(GlobalMaxPooling1D())\n",
    "emotionCNNModel.add(Dense(256, activation='relu'))\n",
    "emotionCNNModel.add(Dense(5, activation='softmax'))\n",
    "emotionCNNModel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "emotionCNNModel.fit(x_train_seq, onehotEncoding.GetOneHotEncodedMatrix(y_train), validation_data=(x_val_seq, onehotEncoding.GetOneHotEncodedMatrix(y_validation)), epochs=5, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aravind\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.359167\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = emotionCNNModel.evaluate(x_train_seq, onehotEncoding.GetOneHotEncodedMatrix(y_train), verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Validation\n",
    "x_test_tok_val = tokenizer.texts_to_sequences(x_test)\n",
    "x_test_seq = pad_sequences(x_test_tok_val, maxlen=vectorLenth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the sequenced values\n",
    "predictedModel = emotionCNNModel.predict(x_test_seq)\n",
    "# Finding the class from predicted values\n",
    "labelledPredictedModel = np.argmax(predictedModel, axis=-1)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting encoded values to actual classes\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelEncoder = LabelEncoder()\n",
    "labelEncoded = labelEncoder.fit_transform(emotions)\n",
    "classPredictedModel = labelEncoder.inverse_transform(labelledPredictedModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9146666666666666\n",
      "[[563  23   8  26   2]\n",
      " [ 13 500   6  13  13]\n",
      " [  5   3 568  17  12]\n",
      " [ 11   5   6 556   0]\n",
      " [  7  67  14   5 557]]\n"
     ]
    }
   ],
   "source": [
    "#Analysing performance metrics\n",
    "from DataClean import PerformanceMetrices\n",
    "modelMetrices = PerformanceMetrices()\n",
    "print(modelMetrices.Accuracy(classPredictedModel,y_test))\n",
    "print(modelMetrices.Confusion_matrix(classPredictedModel,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision , recall , fbetascore, support = precision_recall_fscore_support(y_test, classPredictedModel, average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.9160559311173954\n",
      "Recall : 0.9148886948490013\n",
      "FScore : 0.9143122463861462\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision : \" + str(precision) +\"\\nRecall : \" + str(recall) + '\\nFScore : ' + str(fbetascore) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC score : 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(y_test)\n",
    "y_test = lb.transform(y_test)\n",
    "y_pred = lb.transform(classPredictedModel)\n",
    "print(\"ROC score : \" + str(roc_auc_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference : https://stackoverflow.com/questions/45332410/sklearn-roc-for-multiclass-classification\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(5):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot of a ROC curve for a specific class\n",
    "for i in range(5):\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
